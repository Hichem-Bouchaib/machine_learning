{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The code below is inspired from the link you put on the board: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "\n",
    "We built our model with only 2 classes: bottles and cans. Indeed, with this lab, we learnt that \n",
    "pre-processing the data takes a LONG LONG LONG LONG time. We actually downloaded the images with imagenet and the program\n",
    "is available in the public drive. However, a lot of corrumpted images appeared, so cleaning all of them was creepy.\n",
    "\n",
    "We have 700 images for each class for the training.\n",
    "We have 290 images for each class for the validation.\n",
    "\n",
    "The final accuracy depends on the number of epoch we put. If we put 50, the final accuracy is about 80%.\n",
    "We know that if we use another archiecture of our neurons (as VGG16 that you mentionned), we will get a better accuracy.\n",
    "The problem is that it was quite difficult for our pc to run the program, so that's why we focused on the convnet.\n",
    "'''\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")  #ignore warnings about \"Possibly corrupt EXIF data\" -> images...\n",
    "\n",
    "\n",
    "# we put 150x150 as dimension of all our images. So thet all have the same dimensions\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "train_data_dir = 'data2/train'\n",
    "validation_data_dir = 'data2/validation'\n",
    "nb_train_samples = 1400\n",
    "nb_validation_samples = 580\n",
    "epochs = 50\n",
    "batch_size = 10 #We put 12 as batch_size that contains the number of trains set \n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "    \n",
    "    \n",
    "#Here we build our CNN with 4 layers\n",
    "    \n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape)) #the output is 32x150x150\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(32, (3, 3))) #32 filters of shape 3x3\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3))) # 64 filters of shape 3x3\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten()) #it converts our from 3D to 1D\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', #our loss\n",
    "              optimizer='rmsprop', #we used rmsprop as optimizer for the backward. It is a kind of gradient descent optimization algorithms -> minimize the loss\n",
    "              metrics=['accuracy']) #useful to determine the accuracy\n",
    "\n",
    "# because we do not have a lot of image, we actually generate some other thanks to the existing images (we just \n",
    "#change the zoom, the scale...)\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "#we used images from the target path to generate our batch for the train\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')#because we consider 2 classes, we use binary mode\n",
    "\n",
    "#quite the same here but for validation this time\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary') #because we consider 2 classes, we use binary mode\n",
    "\n",
    "#we use these 2 generators to fit our model. Basically, for each epoch, we put each batch in the network.\n",
    "#because the batch_size is 12 and we have 1400 samples for the train, for each epoch, we actually put 140 batchs in the network\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)\n",
    "\n",
    "model.save_weights('first_try.h5') #as mentionned in the tutorial, we shall always save our wrights after the training\n",
    "\n",
    "'''\n",
    "CONCLUSION:\n",
    "When our data teachers told us that data scientist spend 80% of their time cleaning the data, we did not realize that\n",
    "they were so much right.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
